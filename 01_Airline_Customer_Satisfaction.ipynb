{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Passenger Satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---------------------------------------------\n",
    "* Project Methodology  \n",
    "    - 01. Import CPU Python Libraries \n",
    "    - 02. Function Helper\n",
    "    - 03. Import Dataset & Data Description\n",
    "    - 04. Data Understanding\n",
    "    - 05. Select the Featurs\n",
    "    - 06. Data Pre-Processing\n",
    "    - 07. Exploratory Data Analysis\n",
    "    - 08. Data Transformation\n",
    "    - 9. Feature Selection\n",
    "    - 10. Feature Engineering \n",
    "    - 11. Statistics\n",
    "    - 12. Resampling Data\n",
    "    - 13. Data Splitting \n",
    "    - 14. Machine Learning Models \n",
    "    - 15. Accuracy Score Summary \n",
    "---------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Import CPU Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "np.iinfo(np.uint64).max\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "from termcolor import colored \n",
    "import seaborn as sns  \n",
    "from tabulate import tabulate\n",
    "\n",
    "# Importing plotly and cufflinks in offline mode\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "# Figure&Display options\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le_orig = LabelEncoder()\n",
    "\n",
    "# Data Transformation \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature Selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Import Resampling Library\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Data Splitting \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature Selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# sklearn Classifiers Evaluation libraries\n",
    "from sklearn.metrics import classification_report # To get classification report\n",
    "from sklearn.metrics import confusion_matrix # To get the confusion matrix\n",
    "from sklearn.metrics import accuracy_score # To get the accuracy score \n",
    "\n",
    "# Supervised Machine Learning Models\n",
    "\n",
    "## Random Forest Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Gradient Boosting Classifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "## Histogram-based Gradient Boosting Classification Tree\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "## AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "## Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "## K-Nearest Neighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## Naive Bayes Classifiers\n",
    "from sklearn.naive_bayes import GaussianNB # DV\n",
    "## Naive Bayes classifier for multivariate Bernoulli models\n",
    "from sklearn.naive_bayes import BernoulliNB # 2 | 3 DV\n",
    "\n",
    "\n",
    "## Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## Logistic Regression Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## Logistic Regression CV classifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "## Linear classifiers with stochastic gradient descent SGD training.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "## Linear Perceptron Classifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "## XGBoost Classifiers\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "## Support Vector Machines Classifiers\n",
    "from sklearn.svm import SVC\n",
    "## Linear Support Vector Classification\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "## Multilayer Perceptron Classifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Function Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Drop Varibales\n",
    "'''\n",
    "def DropVariables(dfDrop, col):\n",
    "      dfDrop = dfDrop.drop(col, axis=1)\n",
    "\n",
    "      return dfDrop\n",
    "\n",
    "'''\n",
    "Convert Data Type\n",
    "'''\n",
    "def Convert_Data_Typt_to_str(Con_df_str, col):\n",
    "      for colu in col :\n",
    "            Con_df_str[colu] = Con_df_str[colu].apply(str)\n",
    "            print ('\\nData Type Changed to Objective for Variable: [', colu , '] Data type now is: ' , Con_df_str[colu].dtype)\n",
    "\n",
    "      return Con_df_str\n",
    "      \n",
    "'''\n",
    "Missong Value Information\n",
    "'''\n",
    "def missing_values(df_missing_value_per):\n",
    "      missing_number = df_missing_value_per.isnull().sum().sort_values(ascending=False)\n",
    "      missing_percent = (df_missing_value_per.isnull().sum()/df_missing_value_per.shape[0]).sort_values(ascending=False) \n",
    "      missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n",
    "\n",
    "      return missing_values[missing_values['Missing_Number']>=0]\n",
    "\n",
    "def missing_values_info(df_missing_value):\n",
    "      print(colored(f\"Missing Values for Catuogirical Dataset:\\n\", attrs=['bold']), missing_values(df_missing_value),'\\n',  \n",
    "            colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "\n",
    "def SelectRowsHavingMissing_Value(df_Missing_Rows):\n",
    "      null_data = df_Missing_Rows[df_Missing_Rows.isnull().any(axis=1)]\n",
    "      \n",
    "      return null_data\n",
    "\n",
    "'''\n",
    "Split the Dataset Variables to Obj and Num\n",
    "'''\n",
    "def Data_Splitting_Num_Obj(df_split):\n",
    "      Obj = df_split.select_dtypes(include = ['object'])\n",
    "      Num = df_split.select_dtypes(include = np.number) \n",
    "\n",
    "      return Obj, Num\n",
    "\n",
    "'''\n",
    "Exploratory Data Analysis (EDA)\n",
    "'''\n",
    "def EDA_target(df_EDA,target):\n",
    "      if df_EDA[target].dtype=='object':\n",
    "            print(df_EDA[target].value_counts())\n",
    "            plt.figure(figsize=(10,10))\n",
    "            explode = [0.1,0.1]\n",
    "            plt.pie(df_EDA[target].value_counts(), explode=explode,autopct='%1.1f%%', shadow=True,startangle=140)\n",
    "            plt.title(target)\n",
    "            plt.axis('off');\n",
    "\n",
    "def obj_EDA(df_EDA, cols):\n",
    "      for col in cols:\n",
    "            table = pd.DataFrame(df_EDA[col].value_counts())\n",
    "            print(tabulate(table, headers = 'keys', tablefmt = 'psql'))\n",
    "            plt.figure(figsize = (8,5))\n",
    "            df_EDA[col].value_counts(normalize = True).plot(kind='bar', color= ['darkorange','steelblue'], alpha = 0.9, rot=0)\n",
    "            plt.title(col)\n",
    "            plt.show()\n",
    "\n",
    "def EDA_obj(df_EDA, cols, Target):\n",
    "      for col in cols:\n",
    "            print('Variable Name: ', df_EDA[col].name)\n",
    "            pd.crosstab(df_EDA[col], df_EDA[Target]).iplot(kind=\"bar\")\n",
    "\n",
    "def EDA_num(df_EDA, cols):\n",
    "    for col in cols: \n",
    "      fig, ax = plt.subplots(figsize=(20, 10))\n",
    "      df_EDA.hist(column=[col], ax=ax  )\n",
    "    df_EDA[cols].plot.box( figsize=(20, 10))\n",
    "    df_EDA[cols].plot(subplots=True, figsize=(20, 10))\n",
    "    \n",
    "def EDA_num_with_DV(df_eda, NUMcols_eda): \n",
    "    for col in NUMcols_eda:\n",
    "        sns.set_style('whitegrid')\n",
    "        plt.figure(figsize=(20,10))\n",
    "        sns.set_context('paper', font_scale=1.5)\n",
    "\n",
    "        sns.histplot(x=col, data = df_eda, bins = 30, hue =target,palette=\"Blues\", kde = True).set_title(col,fontsize=20)\n",
    "        plt.legend(['not satisfaction','satisfaction'],shadow = True, loc = 0);\n",
    "\n",
    "'''\n",
    "Feature Selection\n",
    "'''\n",
    "def Feature_selection_forward(df_forward, target):\n",
    "      ## Split the df to Obj and num\n",
    "      obj, num = Data_Splitting_Num_Obj(df_forward)\n",
    "      list_df_num = list(num.columns)\n",
    "      list_df_num.append(target)\n",
    "      print('All the Num Variables:',list_df_num)\n",
    "      list_df_Obj = list(obj.columns)\n",
    "      print('\\nAll the obj Variables:',list_df_Obj)\n",
    "\n",
    "      # Create New DataFrame Hvae only the Num Variables\n",
    "      df_num = df_forward[list_df_num]\n",
    "\n",
    "      # define dataset\n",
    "      X = df_num.drop(target, axis=1)\n",
    "      y = df_num[target]\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "      forward_FS = SequentialFeatureSelector(RandomForestClassifier(n_jobs= -1, ), # How many course you want to use. '-1 mwans all the course'\n",
    "                                                                  k_features= (1, X.shape[1]),\n",
    "                                                                  forward=True,\n",
    "                                                                  floating=False,\n",
    "                                                                  verbose=2,\n",
    "                                                                  scoring='accuracy',\n",
    "                                                                  cv= 5\n",
    "                  ).fit(X_train, y_train)\n",
    "\n",
    "      print ('Most Variables Can Effect The Target Variables:\\n',forward_FS.k_feature_names_)\n",
    "      print ('\\nWith Highest Score:\\n',forward_FS.k_score_)\n",
    "\n",
    "      new_list_num = list(forward_FS.k_feature_names_)\n",
    "      print('All the Num Variables Selected:',new_list_num)\n",
    "\n",
    "      new_df_list = new_list_num + list_df_Obj\n",
    "      df_forward = df_forward[new_df_list]\n",
    "\n",
    "      forward_FS = pd.DataFrame(forward_FS.get_metric_dict()).T\n",
    "\n",
    "      return df_forward, forward_FS\n",
    "\n",
    "'''\n",
    "Data Transformation\n",
    "'''\n",
    "def NumStandardScaler(dataframe_series):\n",
    "      for col in list(dataframe_series.columns):\n",
    "            if (dataframe_series[col].dtype == 'float64' or dataframe_series[col].dtype == 'int64'):\n",
    "                  print ('\\nStandardization Applied On:', col)\n",
    "                  dataframe_series[col] = StandardScaler().fit_transform(dataframe_series[col].values.reshape(-1,1))\n",
    "\n",
    "      return dataframe_series\n",
    "\n",
    "'''\n",
    "Feature Engineering \n",
    "'''\n",
    "def AllObjLabelEncoder(dataframe_series):\n",
    "      if dataframe_series.dtype=='object':\n",
    "            print('\\nLableEncoding Applied On:', dataframe_series.name)\n",
    "            dataframe_series = LabelEncoder().fit_transform(dataframe_series)\n",
    "            \n",
    "\n",
    "      return dataframe_series\n",
    "      \n",
    "'''\n",
    "Resampling Data\n",
    "'''\n",
    "def resampling_by_SMOTE(x_s, y_s): \n",
    "      model = DecisionTreeClassifier()\n",
    "      cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "      # define SMOTE\n",
    "      smote = SMOTE() \n",
    "      # fit predictor and target variable\n",
    "      x_smote, y_smote = smote.fit_resample(x_s, y_s)\n",
    "      # summarize the new class distribution\n",
    "      print('\\nOriginal dataset shape', Counter(y_s))\n",
    "      print('SMOTE Resample dataset shape', Counter(y_smote))\n",
    "      # Evaluate pipeline\n",
    "      scores_SMOTE = cross_val_score(model, x_smote, y_smote, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "      scores_SMOTE = mean(scores_SMOTE)\n",
    "      print('SMOTE Mean ROC AUC: ', scores_SMOTE)\n",
    "\n",
    "\n",
    "      # Define SMOTESVM\n",
    "      oversample = SVMSMOTE()\n",
    "      # fit predictor and target variable\n",
    "      X_svm, y_svm = oversample.fit_resample(x_s, y_s)\n",
    "      # Summarize the new class distribution\n",
    "      print('\\nOriginal dataset shape', Counter(y_s))\n",
    "      print('SMOTE SVM Resample dataset shape', Counter(y_svm))\n",
    "      # Evaluate pipeline\n",
    "      scores_SVM = cross_val_score(model, X_svm, y_svm, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "      scores_SVM = mean(scores_SVM)\n",
    "      print('SMOTE SVM Mean ROC AUC: ', scores_SVM)\n",
    "\n",
    "\n",
    "      # Define Standerd SMOTE pipeline\n",
    "      under = RandomUnderSampler()\n",
    "      over = SMOTE()\n",
    "      steps = [('over', over), ('u', under)]\n",
    "      pipeline = Pipeline(steps=steps)\n",
    "      # transform the dataset\n",
    "      x_smote_pip, y_smote_pip = pipeline.fit_resample(x_s, y_s)\n",
    "      # summarize the new class distribution\n",
    "      print('\\nOriginal dataset shape', Counter(y_s))\n",
    "      print('Standerd SMOTE pipeline Resample dataset shape', Counter(y_smote_pip))\n",
    "      # Evaluate pipeline\n",
    "      scores_pip = cross_val_score(model, x_smote_pip, y_smote_pip, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "      scores_pip = mean(scores_pip)\n",
    "      print('Standerd SMOTE pipeline Mean ROC AUC: ', scores_pip)\n",
    "\n",
    "\n",
    "      # Define K neighbors SMOTE pipeline\n",
    "      over = SMOTE(k_neighbors=3)\n",
    "      steps = [('over', over), ('u', under)]\n",
    "      pipeline = Pipeline(steps=steps)\n",
    "      # transform the dataset\n",
    "      x_smote_k, y_smote_k = pipeline.fit_resample(x_s, y_s)\n",
    "      # summarize the new class distribution\n",
    "      print('\\nOriginal dataset shape', Counter(y_s))\n",
    "      print('K neighbors SMOTE pipeline Resample dataset shape', Counter(y_smote_k))\n",
    "      # Evaluate pipeline\n",
    "      scores_pip_K = cross_val_score(model, x_smote_k, y_smote_k, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "      scores_pip_K = mean(scores_pip_K)\n",
    "      print('K neighbors SMOTE pipeline Mean ROC AUC: ', scores_pip_K)\n",
    "\n",
    "\n",
    "      if ( (scores_SMOTE >= scores_SVM) and (scores_SMOTE >= scores_pip) and (scores_SMOTE >= scores_pip_K) ): \n",
    "            print('\\n\\tThe Highest ROC Score is: ', scores_SMOTE)\n",
    "            return x_smote, y_smote\n",
    "      elif ( (scores_SVM >= scores_SMOTE) and (scores_SVM >= scores_pip) and (scores_SVM >= scores_pip_K) ): \n",
    "            print('\\n\\tThe Highest ROC Score is: ', scores_SVM)\n",
    "            return X_svm, y_svm\n",
    "      elif ( (scores_pip >= scores_SMOTE) and (scores_pip >= scores_SVM) and (scores_pip >= scores_pip_K) ): \n",
    "            print('\\n\\tThe Highest ROC Score is: ', scores_pip)\n",
    "            return x_smote_pip, y_smote_pip\n",
    "      elif ( (scores_pip_K >= scores_SMOTE) and (scores_pip_K >= scores_SVM) and (scores_pip_K >= scores_pip) ): \n",
    "            print('\\n\\tThe Highest ROC Score is: ', scores_pip_K)\n",
    "            return  x_smote_k, y_smote_k\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Import Dataset & Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/rbani/OneDrive/Desktop/ML DL NLP/Classification/Airline Passenger Satisfaction/airline_passenger_satisfaction.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset contains about 129880 survey entries and passenger/flight details from a US airline. the dataset collocted from https://www.kaggle.com/datasets/mysarahmadbhat/airline-passenger-satisfaction. In total, there are 24 feature columns including binary, object, int, and float data type. Out of all the features, 14 are survey entries where passengers rate the flight experience on a scale of 0 to 5. However, After removing NaN values, the resulting data set for model building has about 129,487 entries.\n",
    "\n",
    "IDV Variables Description: \n",
    "\n",
    "- Gender: male or female\n",
    "- Customer type: regular or non-regular airline customer\n",
    "- Age: the actual age of the passenger\n",
    "- Type of travel: the purpose of the passenger's flight (personal or business travel)\n",
    "- Class: business, economy, economy plus\n",
    "- Flight distance\n",
    "- Inflight wifi service: satisfaction level with Wi-Fi service on board (0: not rated; 1-5)\n",
    "- Departure/Arrival time convenient: departure/arrival time satisfaction level (0: not rated; 1-5)\n",
    "- Ease of Online booking: online booking satisfaction rate (0: not rated; 1-5)\n",
    "- Gate location: level of satisfaction with the gate location (0: not rated; 1-5)\n",
    "- Food and drink: food and drink satisfaction level (0: not rated; 1-5)\n",
    "- Online boarding: satisfaction level with online boarding (0: not rated; 1-5)\n",
    "- Seat comfort: seat satisfaction level (0: not rated; 1-5)\n",
    "- Inflight entertainment: satisfaction with inflight entertainment (0: not rated; 1-5)\n",
    "- On-board service: level of satisfaction with on-board service (0: not rated; 1-5)\n",
    "- Leg room service: level of satisfaction with leg room service (0: not rated; 1-5)\n",
    "- Baggage handling: level of satisfaction with baggage handling (0: not rated; 1-5)\n",
    "- Checkin service: level of satisfaction with checkin service (0: not rated; 1-5)\n",
    "- Inflight service: level of satisfaction with inflight service (0: not rated; 1-5)\n",
    "- Cleanliness: level of satisfaction with cleanliness (0: not rated; 1-5)\n",
    "- Departure delay in minutes\n",
    "- Arrival delay in minutes\n",
    "\n",
    "DV Variable Description: \n",
    "\n",
    "- Satisfaction\n",
    "- Neutral or dissatisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info(), '\\n', \n",
    "            colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "missing_values_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMcols = df.select_dtypes(np.number).columns\n",
    "Objcols = df.select_dtypes(include = ['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(NUMcols) != 0 :\n",
    "    su_stat = pd.DataFrame(df.describe().T)\n",
    "    print(tabulate(su_stat, headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Objcols) != 0 :\n",
    "    su_stat = pd.DataFrame(df.describe(include=object).T)\n",
    "    print(tabulate(su_stat, headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After First Looking into the dataset it found that: \n",
    "- The 'ID' Variance its unuseful Variable. \n",
    "- In addtion to that it found, 'Departure and Arrival Time Convenience', 'Ease of Online Booking', 'Check-in Service', 'Online Boarding', 'Gate Location', 'On-board Service', 'Seat Comfort', 'Leg Room Service', 'Cleanliness', 'Food and Drink', 'In-flight Service', 'In-flight Wifi Service', 'In-flight Entertainment', 'Baggage Handling' in int data type, and it shoud be in object data trpe becuse its a Ordinal level. \n",
    "- There is 393 Missing value 'NaN' in Arrival Delay Variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Select The Featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                1)        Select the Target Varibale\n",
    "'''\n",
    "target = 'Satisfaction'\n",
    "\n",
    "\n",
    "'''\n",
    "                2)        Select the Varibales Dont have any Value \n",
    "'''\n",
    "col_drop = [ 'ID'\n",
    "         \n",
    "        ]\n",
    "\n",
    "'''\n",
    "                3)        Select the Variables In the Wrong Data Type \n",
    "'''\n",
    "# To convert variable type to str\n",
    "col_convert_str = [ 'Departure and Arrival Time Convenience', 'Ease of Online Booking', 'Check-in Service',\n",
    "                        'Online Boarding', 'Gate Location', 'On-board Service', 'Seat Comfort', 'Leg Room Service', \n",
    "                        'Cleanliness', 'Food and Drink', 'In-flight Service', 'In-flight Wifi Service', 'In-flight Entertainment', \n",
    "                        'Baggage Handling'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foolowing what it found in the Data Understaning part, this suction will incloud: \n",
    "- Drop Variable: 'ID'\n",
    "- Convert data type for 'Departure and Arrival Time Convenience', 'Ease of Online Booking', 'Check-in Service', 'Online Boarding', 'Gate Location', 'On-board Service', 'Seat Comfort', 'Leg Room Service', 'Cleanliness', 'Food and Drink', 'In-flight Service', 'In-flight Wifi Service', 'In-flight Entertainment', 'Baggage Handling' from int to object\n",
    "- Drop Missing Value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DropVariables(df, col_drop)\n",
    "\n",
    "print(tabulate(df.info(), headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Convert_Data_Typt_to_str(df,col_convert_str)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMcols = df.select_dtypes(np.number).columns\n",
    "Objcols = df.select_dtypes(include = ['object']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.reset_index()\n",
    "df = df.drop(columns=['index'])\n",
    "missing_values_info(df)\n",
    "print(\"Dataset size after remove all the missing value: \",df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMcols = df.select_dtypes(np.number).columns\n",
    "\n",
    "Objcols = df.select_dtypes(include = ['object']).columns\n",
    "Objcols = Objcols.drop(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_target(df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pie chart above represents the two possible prediction oucomes of the machine learning models to be developed. As it can be seen, the dataset consists of an imbalance in terms of its possible outcomes of 'Neutral or Dissatisfied' and 'Satisfied. \n",
    "This is a crucial insight as the the imbalance needs to be corrected in order to prevent overfitting of the machine learning model to be developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDV Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_EDA(df, Objcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDV Objective With DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_obj(df, Objcols, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDV Numrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_num(df, NUMcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDV Numrical With DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_num_with_DV(df, NUMcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning algorithms perform better when numerical input variables are scaled to a standard range.\n",
    "Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "df = NumStandardScaler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the table above, standardisation was performed to the variables \"Age,\" \"Flight Distance,\" \"Departure Delay,\" and \"Arrival Delay.\" each variable's value has previously been standardised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection helps in finding the smallest set of features which results in\n",
    "\n",
    "- Training a machine learning algorithm faster.\n",
    "- Reducing the complexity of a model and making it easier to interpret.\n",
    "- Building a sensible model with better prediction power.\n",
    "- Reducing over-fitting by selecting the right set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper \"Forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, fs = Feature_selection_forward(df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see In forward selection, it starts with a null model and then starts fitting the model with each individual numerical feature one at a time and selects the feature with the minimum p-value. then it fits a model with two features by trying combinations of the earlier selected feature with all other remaining features. Again it selects the feature with the minimum p-value. then it fits a model with three features by trying combinations of two previously selected features with other remaining features. It repeats this process until it has a set of selected features with a p-value of individual features less than the significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Flight Distance' variable, which has the highest average score and has no other numerical variables, is shown to have the highest average score in the table above that displays the average score for each step in the forward selection.\n",
    "Therefore, out of all the numerical features, the model will now just choose the 'Flight Distance' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is very simple and it involves converting each value in a column to a number. \n",
    "As well as, it requires the category column to be of ‘category’ datatype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: AllObjLabelEncoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between IDV and DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.corr()[target].sort_values(ascending = False).reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between all the Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().style.background_gradient(cmap=\"Blues\") # YlOrBr Greys GnBu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Resampling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance the minority class. In this section will develop an intuition for the SMOTE approaches by applying it to an imbalanced binary classification problem.\n",
    "- Note the model will evaluate using the ROC area under curve (AUC) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X, y\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "X, y = resampling_by_SMOTE(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it provides, the K neighbors SMOTE pipeline is showing the highest ROC, so it the SMOTE approach select is K neighbors SMOTE pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nthe new target size now after resampling is: ', Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Data Splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataset size, it will split the dataset into 90% for traning and 10% for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.1, random_state=42)\n",
    "\n",
    "\n",
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Machine Learning Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this sectuion going to fit 'Train' 16 machine learning models:\n",
    "    -        Random Forest Classifier\n",
    "    -        Gradient Boosting Classifier\n",
    "    -        Histogram-based Gradient Boosting Classification Tree\n",
    "    -        AdaBoost Classifier\n",
    "    -        Extra Trees Classifier\n",
    "    -        K Neighbors Classifier\n",
    "    -        Naive Bayes Classifiers\n",
    "    -        Naive Bayes Classifier for Multivariate Bernoulli\n",
    "    -        Decision Tree Classifier\n",
    "    -        Logistic Regression Classifier\n",
    "    -        Logistic Regression CV Classifier\n",
    "    -        Stochastic Gradient Descent Classifier\n",
    "    -        Linear Perceptron Classifier\n",
    "    -        XGBoost Classifiers\n",
    "    -        Support Vector Machines Classifiers\n",
    "    -        Linear Support Vector Classification\n",
    "    -        Multilayer Perceptron Classifier\n",
    "* Following that it going to test the models.\n",
    "\n",
    "* After that it going to evaluate each model using: \n",
    "    -   Accuracy Score\n",
    "    -   Classification Report\n",
    "    -   Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Machine Learning Models on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf_m_cpu = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_m_cpu = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Histogram-based Gradient Boosting Classification Tree\n",
    "hgb_m_cpu = HistGradientBoostingClassifier().fit(X_train, y_train)\n",
    "\n",
    "# AdaBoost Classifier\n",
    "ad_m_cpu = AdaBoostClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Extra Trees Classifier\n",
    "et_m_cpu = ExtraTreesClassifier().fit(X_train, y_train)\n",
    "\n",
    "# K Neighbors Classifier\n",
    "knn_m_cpu = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Naive Bayes Classifiers\n",
    "nb_m_cpu = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "# Naive Bayes Classifier for Multivariate Bernoulli\n",
    "bnb_m_cpu = BernoulliNB().fit(X_train, y_train)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt_m_cpu = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "lg_m_cpu = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Logistic Regression CV Classifier\n",
    "lgcv_m_cpu = LogisticRegressionCV().fit(X_train, y_train)\n",
    "\n",
    "# Stochastic Gradient Descent Classifier\n",
    "sgdc_m_cpu = SGDClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Linear Perceptron Classifier\n",
    "lpc_m_cpu = Perceptron().fit(X_train, y_train)\n",
    "\n",
    "# XGBoost Classifiers\n",
    "xgb_m_cpu = XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Support Vector Machines Classifiers\n",
    "svm_m_cpu = SVC().fit(X_train, y_train)\n",
    "\n",
    "# Linear Support Vector Classification\n",
    "lsvm_m_cpu = LinearSVC().fit(X_train, y_train)\n",
    "\n",
    "# Multilayer Perceptron Classifier\n",
    "mlp_m_cpu = MLPClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict y_test Using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf_m_pred_vaild_cpu = rf_m_cpu.predict(X_test)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_m_pred_vaild_cpu = gb_m_cpu.predict(X_test)\n",
    "\n",
    "# Histogram-based Gradient Boosting Classification Tree\n",
    "hgb_m_pred_vaild_cpu = hgb_m_cpu.predict(X_test)\n",
    "\n",
    "# AdaBoost Classifier\n",
    "ad_m_pred_vaild_cpu = ad_m_cpu.predict(X_test)\n",
    "\n",
    "# Extra Trees Classifier\n",
    "et_m_pred_vaild_cpu = et_m_cpu.predict(X_test)\n",
    "\n",
    "# K Neighbors Classifier\n",
    "knn_m_pred_vaild_cpu = knn_m_cpu.predict(X_test)\n",
    "\n",
    "# Naive Bayes Classifiers\n",
    "nb_m_pred_vaild_cpu = nb_m_cpu.predict(X_test)\n",
    "\n",
    "# Naive Bayes Classifier for Multivariate Bernoulli\n",
    "bnb_m_pred_vaild_cpu = bnb_m_cpu.predict(X_test)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt_m_pred_vaild_cpu = dt_m_cpu.predict(X_test)\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "lg_m_pred_vaild_cpu = lg_m_cpu.predict(X_test)\n",
    "\n",
    "# Logistic Regression CV Classifier\n",
    "lgcv_m_pred_vaild_cpu =  lgcv_m_cpu.predict(X_test)\n",
    "\n",
    "# Stochastic Gradient Descent Classifier\n",
    "sgdc_m_pred_vaild_cpu =  sgdc_m_cpu.predict(X_test)\n",
    "\n",
    "# Linear Perceptron Classifier\n",
    "lpc_m_pred_vaild_cpu =  lpc_m_cpu.predict(X_test)\n",
    "\n",
    "# XGBoost Classifiers\n",
    "xgb_m_pred_vaild_cpu = xgb_m_cpu.predict(X_test)\n",
    "\n",
    "# Support Vector Machines Classifiers\n",
    "svm_m_pred_vaild_cpu = svm_m_cpu.predict(X_test)\n",
    "\n",
    "# Linear Support Vector Classification\n",
    "lsvm_m_pred_vaild_cpu = lsvm_m_cpu.predict(X_test)\n",
    "\n",
    "# Multilayer Perceptron Classifier\n",
    "mlp_m_pred_vaild_cpu = mlp_m_cpu.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Models CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "Valid_accuracy_rf_m_cpu = accuracy_score(y_test, rf_m_pred_vaild_cpu)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "Valid_accuracy_gb_m_cpu = accuracy_score(y_test, gb_m_pred_vaild_cpu)\n",
    "\n",
    "# Histogram-based Gradient Boosting Classification Tree\n",
    "Valid_accuracy_hgb_m_cpu = accuracy_score(y_test, hgb_m_pred_vaild_cpu)\n",
    "\n",
    "# AdaBoost Classifier\n",
    "Valid_accuracy_ad_m_cpu = accuracy_score(y_test, ad_m_pred_vaild_cpu)\n",
    "\n",
    "# Extra Trees Classifier\n",
    "Valid_accuracy_et_m_cpu = accuracy_score(y_test, et_m_pred_vaild_cpu)\n",
    "\n",
    "# K Neighbors Classifier\n",
    "Valid_accuracy_knn_m_cpu = accuracy_score(y_test, knn_m_pred_vaild_cpu)\n",
    "\n",
    "# Naive Bayes Classifiers\n",
    "Valid_accuracy_nb_m_cpu = accuracy_score(y_test, nb_m_pred_vaild_cpu)\n",
    "\n",
    "# Naive Bayes Classifier for Multivariate Bernoulli\n",
    "Valid_accuracy_bnb_m_cpu = accuracy_score(y_test, bnb_m_pred_vaild_cpu)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "Valid_accuracy_dt_m_cpu = accuracy_score(y_test, dt_m_pred_vaild_cpu)\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "Valid_accuracy_lg_m_cpu = accuracy_score(y_test, lg_m_pred_vaild_cpu)\n",
    "\n",
    "# Logistic Regression CV Classifier\n",
    "Valid_accuracy_lgcv_m_cpu = accuracy_score(y_test, lgcv_m_pred_vaild_cpu)\n",
    "\n",
    "# Stochastic Gradient Descent Classifier\n",
    "Valid_accuracy_sgdc_m_cpu = accuracy_score(y_test, sgdc_m_pred_vaild_cpu)\n",
    "\n",
    "# Linear Perceptron Classifier\n",
    "Valid_accuracy_lpc_m_cpu = accuracy_score(y_test, lpc_m_pred_vaild_cpu)\n",
    "\n",
    "# XGBoost Classifiers\n",
    "Valid_accuracy_xgb_m_cpu = accuracy_score(y_test, xgb_m_pred_vaild_cpu)\n",
    "\n",
    "# Support Vector Machines Classifiers\n",
    "Valid_accuracy_svm_m_cpu = accuracy_score(y_test, svm_m_pred_vaild_cpu)\n",
    "\n",
    "# Linear Support Vector Classification\n",
    "Valid_accuracy_lsvm_m_cpu = accuracy_score(y_test, lsvm_m_pred_vaild_cpu)\n",
    "        \n",
    "# Multilayer Perceptron Classifier\n",
    "Valid_accuracy_mlp_m_cpu = accuracy_score(y_test, mlp_m_pred_vaild_cpu)\n",
    "\n",
    "cpu_ml_m_valid_compare = pd.DataFrame({\"Standered CPU Models\": [\n",
    "                                                \"Random Forest Classifier\", \n",
    "                                                \"Gradient Boosting Classifier\",\n",
    "                                                \"Histogram-based Gradient Boosting Classification Tree\",\n",
    "                                                \"AdaBoost Classifier\",\n",
    "                                                \"Extra Trees Classifier\",\n",
    "                                                \"K Neighbors Classifier\",\n",
    "                                                \"Naive Bayes Classifiers\",\n",
    "                                                \"Naive Bayes Classifier for Multivariate Bernoulli\",\n",
    "                                                \"Decision Tree Classifier\",\n",
    "                                                \"Logistic Regression Classifier\",\n",
    "                                                \"Logistic Regression CV Classifier\",\n",
    "                                                \"Stochastic Gradient Descent Classifier\",\n",
    "                                                \"Linear Perceptron Classifier\",\n",
    "                                                \"XGBoost Classifiers\",\n",
    "                                                \"Support Vector Machines Classifiers\",\n",
    "                                                \"Linear Support Vector Classification\",\n",
    "                                                \"Multilayer Perceptron Classifier\"\n",
    "                                                ],\n",
    "\n",
    "                                \"Accuracy\": [\n",
    "                                                Valid_accuracy_rf_m_cpu, \n",
    "                                                Valid_accuracy_gb_m_cpu,\n",
    "                                                Valid_accuracy_hgb_m_cpu,\n",
    "                                                Valid_accuracy_ad_m_cpu,\n",
    "                                                Valid_accuracy_et_m_cpu,\n",
    "                                                Valid_accuracy_knn_m_cpu,\n",
    "                                                Valid_accuracy_nb_m_cpu,\n",
    "                                                Valid_accuracy_bnb_m_cpu,\n",
    "                                                Valid_accuracy_dt_m_cpu,\n",
    "                                                Valid_accuracy_lg_m_cpu,\n",
    "                                                Valid_accuracy_lgcv_m_cpu,\n",
    "                                                Valid_accuracy_sgdc_m_cpu,\n",
    "                                                Valid_accuracy_lpc_m_cpu,\n",
    "                                                Valid_accuracy_xgb_m_cpu,\n",
    "                                                Valid_accuracy_svm_m_cpu,\n",
    "                                                Valid_accuracy_lsvm_m_cpu,\n",
    "                                                Valid_accuracy_mlp_m_cpu\n",
    "                                                ],\n",
    "                                        })  \n",
    "                                              \n",
    "print(tabulate(cpu_ml_m_valid_compare.sort_values(by=\"Accuracy\", ascending=False), headers = 'keys', tablefmt = 'psql'))\n",
    "fig = px.bar(cpu_ml_m_valid_compare.sort_values(by=\"Accuracy\", ascending=True), x = \"Accuracy\", y = \"Standered CPU Models\", title = \"Accuracy Validation for Machines Learning Model on CPU\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report and Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Random Forest Classifier Validation Classification Report:\\n ', classification_report(y_test, rf_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, rf_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Random Forest Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Gradient Boosting Classifier Validation Classification Report:\\n ', classification_report(y_test, gb_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, gb_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Gradient Boosting Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram-based Gradient Boosting Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Histogram-based Gradient Boosting Classification Tree Validation Classification Report:\\n ', classification_report(y_test, hgb_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, hgb_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Histogram-based Gradient Boosting Classification Tree Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model AdaBoost Classifier Validation Classification Report:\\n ', classification_report(y_test, ad_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, ad_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model AdaBoost Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Extra Trees Classifier Validation Classification Report:\\n ', classification_report(y_test, et_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, et_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Extra Trees Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model K Neighbors Classifier Validation Classification Report:\\n ', classification_report(y_test, knn_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, knn_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model K Neighbors Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Naive Bayes Classifier Validation Classification Report:\\n ', classification_report(y_test, nb_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, nb_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Naive Bayes Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier for Multivariate Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Naive Bayes Classifier for Multivariate Bernoulli Validation Classification Report:\\n ', classification_report(y_test, bnb_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, bnb_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Naive Bayes Classifier for Multivariate Bernoulli Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Decision Tree Classifier Validation Classification Report:\\n ', classification_report(y_test, dt_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, dt_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Decision Tree Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Logistic Regression Classifier Validation Classification Report:\\n ', classification_report(y_test, lg_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, lg_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Logistic Regression Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression CV Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Logistic Regression CV Classifier Validation Classification Report:\\n ', classification_report(y_test, lgcv_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, lgcv_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Logistic Regression CV Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Stochastic Gradient Descent Classifier Validation Classification Report:\\n ', classification_report(y_test, sgdc_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, sgdc_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Stochastic Gradient Descent Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Linear Perceptron Classifier Validation Classification Report:\\n ', classification_report(y_test, lpc_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, lpc_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Linear Perceptron Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model XGBoost Classifier Validation Classification Report:\\n ', classification_report(y_test, xgb_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, xgb_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model XGBoost Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Support Vector Machines Classifier Validation Classification Report:\\n ', classification_report(y_test, svm_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, svm_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Support Vector Machines Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Linear Support Vector Classificationr Validation Classification Report:\\n ', classification_report(y_test, lsvm_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, lsvm_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Linear Support Vector Classification Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Multilayer Perceptron Classifier Validation Classification Report:\\n ', classification_report(y_test, mlp_m_pred_vaild_cpu, digits = 3))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion_matrix(y_test, mlp_m_pred_vaild_cpu),\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "        \n",
    "plt.title(\"Model Multilayer Perceptron Classifier Validation Confusion Matrix\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Accuracy Score Summary  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standered Machien Learning Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predication Using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(cpu_ml_m_valid_compare.sort_values(by=\"Accuracy\", ascending=False), headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(cpu_ml_m_valid_compare.sort_values(by=\"Accuracy\", ascending= True ), x = \"Accuracy\", y = \"Standered CPU Models\", title = \"Machines Learning CPU Accuracy Validation \")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for this project it is showing Random Forest Classifier, Extra Trees Classifier, XGBoost Classifiers with accuracy (96.4, 96.4, and 95.8) respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
